<html>
<head>
<title>How Qualcomm Made Huge Improvements in AI on the Snapdragon 865</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>高通如何在骁龙865人工智能上取得巨大进步</h1>
<blockquote>原文：<a href="https://www.xda-developers.com/qualcomm-snapdragon-865-ai-performance-machine-learning-analysis/#0001-01-01">https://www.xda-developers.com/qualcomm-snapdragon-865-ai-performance-machine-learning-analysis/#0001-01-01</a></blockquote><div><div class="content-block-regular">
<p/><p>似乎我们没有一天不在新闻中看到“人工智能”，过去的一周也不例外，这在很大程度上要归功于骁龙科技峰会。每年，高通都会推出其Hexagon DSP和高通人工智能引擎的大量改进，当谈到人工智能工作负载时，他们使用这个术语来描述他们的整个异构计算平台- CPU，GPU和DSP。几年前，高通坚持将话题从传统话题转移开，比如每年的CPU性能改进，这似乎有点奇怪。然而，在2019年和骁龙865，我们看到异构计算确实是他们移动计算推动的掌舵人，因为人工智能和硬件加速的工作负载似乎悄悄进入了从社交媒体到日常服务的一系列用例和应用程序。</p>

  
<p>骁龙865带来了高通的第五代人工智能引擎，随之而来的是性能和能效的大幅提升——但这是意料之中的。在大量的规格、性能数据、花哨的工程术语和令人厌烦的营销术语中，很容易忽略这些改进的实际意义。他们描述了什么？为什么这些升级对今天在应用中实现人工智能的人如此有意义，也许更重要的是，对那些希望在未来这样做的人如此有意义？</p>

<p>在这篇文章中，我们将深入浅出地浏览高通人工智能引擎，梳理它的历史，它的组件和骁龙865的升级，最重要的是，<em>为什么</em>或<em>如何</em>这些都对今天的智能手机体验做出了贡献，从有趣的滤镜到数字助理。</p>

<h3 id="the-hexagon-dsp-and-qualcomm-ai-engine-when-branding-makes-a-difference">Hexagon DSP和高通人工智能引擎:当品牌有所作为</h3>
<p>虽然我没能参加本周的骁龙科技峰会，但我还是参加了自2015年以来的每一届峰会。如果你还记得，<em/>是Snapdragon 810最混乱的一年，因此在纽约市切尔西阁楼的记者们急于知道Snapdragon 820将如何拯救公司。这是一个伟大的芯片组，好吗:它承诺健康的性能改善(没有节流)，通过回到当时久经考验的高通著名的定制核心。然而，我还记得一个非常微妙的公告，回想起来，应该得到更多的关注:第二代Hexagon 680 DSP及其单指令，多数据(SIMD) Hexagon Vector eXtensions，或HVX。也许如果工程师们没有命名这个特性，它会得到应有的关注。</p>

<p>该协处理器允许标量DSP单元的硬件线程访问HVX“上下文”(寄存器文件)，以获得广泛的矢量处理能力。它能够将大量计算工作负载从耗电的CPU或GPU转移到高能效的DSP，从而大幅提高成像和计算机视觉任务的性能功耗比。它们非常适合对连续的向量元素(最初只是整数)应用相同的操作，非常适合计算机视觉工作负载。<a href="https://www.xda-developers.com/qualcomm-snapdragon-845-hexagon-685-dsp/">我们在过去写了一篇关于DSP和HVX的深入文章</a>，指出HVX架构非常适合并行化，显然，也非常适合处理大型输入向量。当时，高通几乎完全是通过描述DSP和HVX将给计算机视觉工作负载带来的改进来推广DSP和HVX，如哈里斯边角侦测和其他滑动窗口方法。</p>

<p>直到深度学习在消费移动应用中的出现，DSP、其矢量处理单元(现在是张量加速器)才会与人工智能和神经网络特别是结合起来。但回过头来看，这非常有意义:数字信号处理器(DSP)架构最初是为处理数字化的真实世界或模拟信号输入而设计的，它适用于许多与许多机器学习算法和神经网络相同的工作负载。例如，DSP专为滤波器内核、卷积和相关运算、8位计算、大量线性代数(矢量和矩阵乘积)和乘累加(MAC)运算量身定制，在并行化时效率最高。神经网络的运行时间也高度依赖于大向量、矩阵和/或张量的乘法运算，因此DSP的性能优势自然也能完美地转化为神经网络架构。总之我们会重温这个话题！</p>

<p>在随后的几年里，高通继续强调他们提供的不仅仅是芯片组，而是移动平台，他们关注的不仅仅是改进特定的组件，而是提供“异构”计算。2017年，他们在高通开发者网络上发布了他们的骁龙神经处理引擎SDK(用于运行时加速)，2018年初，他们宣布了高通人工智能引擎，将他们的几个支持人工智能的硬件(CPU，GPU，DSP)和软件组件整合到一个名称下。有了这个有用的术语，他们能够巧妙地宣传他们在骁龙855和骁龙865上的人工智能性能改善，能够轻松地说出每秒万亿次运算的数量(TOPS)和同比百分比改善。利用CPU、GPU和DSP的逐代改进——所有这些都有自己专注于人工智能的升级——该公司能够发布针对竞争对手的令人印象深刻的基准，我们将很快对此进行介绍。随着该公司最近的营销努力和对异构计算的统一、一致的信息传递，他们的人工智能品牌终于在记者和技术爱好者中获得了牵引力。</p>

<h3 id="demystifying-neural-networks-a-mundane-pile-of-linear-algebra">揭开神经网络的神秘面纱:一堆平凡的线性代数</h3>
<p>为了解开我们将在本文后面遇到的许多术语，我们需要一个简短的入门，介绍一下什么是神经网络和你需要什么来使它更快。我想非常简要地回顾一下神经网络的一些数学基础，尽量避免使用术语和符号。本节的目的只是简单地识别神经网络正在做什么，从根本上来说:<strong>它执行的算术运算</strong>，而不是证明所述运算的理论基础(那要复杂得多！).如果你想直接跳到高通AI引擎升级，请随意进入下一部分。</p>

<p><a href="https://www.xda-developers.com/qualcomm-snapdragon-865-ai-performance-machine-learning-analysis/image5-15/" rel="attachment wp-att-287315"/></p>



<p/>

<p/><p><em>“向量数学是深度学习的基础。”–高通产品管理高级总监Travis Lanier在2017年骁龙科技峰会上</em></p>

<p>下面你会发现一个非常典型的前馈全连接神经网络图。实际上，这个图让整个过程看起来比实际要复杂一些(至少在你习惯之前是这样)。我们将计算向前传递，这最终是网络在产生<em>推理</em>时所做的事情，这个术语我们将在本文后面遇到。目前，我们只关心机器及其部件，并对每个部件作简要说明。</p>

<p><a href="https://www.xda-developers.com/qualcomm-snapdragon-865-ai-performance-machine-learning-analysis/neural_network_example_1/" rel="attachment wp-att-287318"/></p>



<p/>

<p>一个神经网络由连续的<em>层</em>组成，每层由几个“神经元”(图中用圆圈表示)组成，这些神经元由<em>权重</em>(图中用线条表示)连接。一般来说，有三种层:<em>输入层</em>，接受原始输入；<em>隐藏层</em>，计算前一层的数学运算，以及<em>输出层</em>，提供最终预测。在这种情况下，我们只有一个隐藏层，有三个隐藏单元。<em>输入</em>由一个特定维度或长度的向量、数组或数字列表组成。在例子中，我们将有一个二维输入，比如说<strong>【1.0，-1.0】</strong>。这里，网络的<em>输出</em>由一个标量或单个数(不是一个列表)组成。每个隐藏单元与一组<em>权重</em>和一个<em>偏置项</em>相关联，显示在每个节点的旁边和下面。为了计算一个单元的<em>加权和</em>输出，将每个权重乘以每个相应的输入，然后将乘积加在一起。然后，我们将简单地将偏置项加到乘积的和上，得到神经元的输出。比如我们的输入为<strong>【1.0，-1.0】</strong>，第一个隐藏单元的输出为<strong>1.0 *<span>0.3</span>+(-1.0)*<span>0.2</span>+<span>1.0</span>= 1.1</strong>。简单吧？</p>

<p>图中的下一步表示一个<em>激活函数</em>，这将允许我们产生每个隐藏层的输出向量。在我们的例子中，我们将使用非常流行和非常简单的<em>整流线性单元</em>或<em> ReLU </em>，它将接受一个输入数并输出(I)零(如果该数为负)或零(ii)输入数本身(如果该数为正)。比如<strong> ReLU(-0.1) = 0 </strong>，但是<strong> ReLU(0.1) = 0.1。</strong>以我们的输入为例，当<em>通过第一个隐藏单元传播</em>时，我们计算的1.1的输出将被传递给激活函数，产生<strong> ReLU(1.1)=1.1 </strong>。在本例中，输出层的功能就像一个隐藏单元:它将隐藏单元的输出乘以其权重，然后添加其偏置项<span> <strong> 0.2 </strong> </span>。最后一个激活功能是<em>阶跃功能</em>，将正输入变为1，负值变为0。知道了网络中的每一个操作是如何操作的，我们可以把我们的推论的完整计算写下来如下:</p>

<p><a href="https://www.xda-developers.com/qualcomm-snapdragon-865-ai-performance-machine-learning-analysis/neural_network_example_2/" rel="attachment wp-att-287319"/></p>



<p/>

<p>这就是我们前馈神经网络计算的全部内容。如您所见，<strong>运算几乎完全由乘积和数字总和</strong>组成。我们的激活函数<em> ReLU(x) </em>也可以非常容易地实现，例如通过简单地调用<em> max(x，0) </em>，使得每当输入大于0时它返回x，否则它返回0。注意<em>步骤(x) </em>可以类似地计算。存在许多更复杂的激活函数，如<em>s形函数</em>或<em>双曲正切函数</em>，涉及不同的内部计算，更适合不同的用途。你可能已经开始注意到的另一件事是，我们也可以并行地<strong>运行三个隐藏单元的计算和它们的ReLU应用</strong>，因为直到我们在输出节点计算它们的加权和之前，它们的值并不同时需要。</p>

<p><a href="https://www.xda-developers.com/qualcomm-snapdragon-865-ai-performance-machine-learning-analysis/neural_network_example_3/" rel="attachment wp-att-287320"/></p>



<p/>

<p>但我们不必就此止步。上面，你可以看到同样的计算，但是这次用矩阵和向量乘法操作来表示。为了达到这种表示，我们通过向输入向量添加1.0(较浅色调)来“增加”输入向量，这样，当我们将权重和偏差(较浅色调)放入如上所示的矩阵中时，所得到的乘法产生相同的隐藏单元输出。然后，我们可以对输出向量应用ReLU，按元素方式，然后“增加”ReLU输出，将其乘以输出层的权重和偏差。这种表示大大简化了符号，因为整个隐藏层的参数(权重和偏差)可以隐藏在单个变量下。但对我们来说最重要的是，它清楚地表明了网络的内部计算本质上是矩阵和向量乘法或点积。考虑到这些向量和矩阵的大小与我们的输入维度和网络中的参数数量成比例，大多数运行时间将用于进行这类计算。一堆线性代数！</p>

<p>当然，我们的玩具例子在范围上非常有限。在实践中，现代深度学习模型可能有数十个甚至数百个隐藏层，以及数百万个相关参数。与我们的二维向量输入示例不同，它们可以接受具有数千个条目的各种形状的向量，如矩阵(如单通道图像)或张量(三通道RGB图像)。通过向原始输入添加行，也没有什么可以阻止我们的矩阵表示一次接受多个输入向量。神经网络也可以与我们的前馈神经网络“连线”不同，或者执行不同的激活功能。有一个庞大的网络架构和技术动物园，但最终，他们<em>大多</em>分解成相同的并行算术运算，我们在我们的玩具例子中发现，只是在一个更大的规模。</p>

<p><a href="https://www.xda-developers.com/qualcomm-snapdragon-865-ai-performance-machine-learning-analysis/convolution_layer_example/" rel="attachment wp-att-287322"/></p>



<p/>

<p/><p><em>对张量进行卷积运算的可视化示例。(图片鸣谢:<a href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53">走向数据科学</a> ) </em></p>

<p>例如，你可能读过的流行的<em>卷积神经网络(CNN)</em>并不像我们的模拟网络那样是“全连接”的。隐藏的<em>卷积层</em>的“权重”或参数可以被认为是一种过滤器，一个滑动窗口，顺序应用于输入的小块，如上图所示——这个“卷积”实际上只是一个滑动点积！这个过程产生了通常所说的<em>特征图。</em>池层通过计算图像小块的最大值或平均值来减小输入或卷积层输出的大小。网络的其余部分通常由完全连接的层组成，就像我们例子中的那些层，以及像ReLU这样的激活功能。这通常用于图像中的特征提取，早期卷积层的特征图可以“检测”线条或边缘等图案，而后期卷积层可以检测更复杂的特征，如面部或复杂形状。</p>

<p>所有所说的都是严格限于推断的<em>，或者在通过<em>训练</em>找到神经网络的参数之后评估神经网络，这是一个复杂得多的过程。再说一次，我们已经排除了很多解释。实际上，网络的每一个组件都是有目的的。例如，你们中学习过线性代数的人可以很容易地观察到，如果没有非线性激活函数，我们的网络会简化为预测能力非常有限的线性模型。</em></p>

<h3 id="an-upgraded-ai-engine-on-the-snapdragon-865---a-summary-of-improvements">骁龙865上升级的人工智能引擎——改进概要</h3>
<p>有了对神经网络的组件及其数学运算的这种方便的理解，我们可以开始理解为什么硬件加速如此重要。在最后一节中，我们可以看到，并行化对于加快网络速度至关重要，因为它允许我们计算与每个神经元激活相对应的多个并行点积。每一个点积本身都是由数字的乘加运算组成的，在移动应用中通常有8位精度，必须尽可能快地进行。AI引擎提供了各种组件来卸载这些任务，具体取决于开发人员对性能和能效的考虑。</p>

<p><a href="https://www.xda-developers.com/qualcomm-snapdragon-865-ai-performance-machine-learning-analysis/qualcomm-ai-engine-blocks/" rel="attachment wp-att-287329"/></p>



<p/>

<p/><p>在今年的骁龙峰会舞台上展示的一张CNN热门MNIST数据集的图表。向量处理单元非常适合完全连接的层，就像我们的模拟示例一样。同时，张量处理器处理并行处理多个滑动内核的卷积层和池层，如上图所示，每个卷积层可能会输出许多单独的特征图。</p>

<p>首先，我们来看看GPU，我们通常在3D游戏的背景下谈论它。几十年来，视频游戏的消费市场刺激了图形处理硬件的发展，但为什么GPU对神经网络如此重要？首先，他们一次咀嚼大量多边形顶点的3D坐标列表，以跟踪游戏中的世界状态。GPU还必须执行巨大的矩阵乘法运算，以将这些3D坐标转换(或映射)到2D平面、屏幕坐标上，并并行处理像素的颜色信息。最重要的是，它们提供了高内存带宽来处理覆盖在游戏几何图形上的纹理位图的大规模内存缓冲区。它在并行化、内存带宽和线性代数能力方面的优势与神经网络的性能要求相匹配。</p>

<p>Adreno GPU系列因此在高通人工智能引擎中发挥了重要作用，在舞台上，高通表示，与上一代产品相比，骁龙865中的这一更新组件使<strong>的浮点功能</strong>和<strong>的数量</strong>增加了一倍，这令人惊讶，因为他们只公布了25%的图形渲染性能提升。尽管如此，在这次发布中，该公司声称算术逻辑单元(ALUs)的数量增加了50%(T5)，尽管像往常一样，他们没有透露他们的GPU频率。高通还列出了混合精度指令，这就像它听起来的一样:在一个计算方法中，不同的运算有不同的数值精度。</p>

<p><a href="https://www.xda-developers.com/qualcomm-snapdragon-865-ai-performance-machine-learning-analysis/snapdragon-865-adreno/" rel="attachment wp-att-287330"/></p>



<p/>

<p>Hexagon 698 DSP是骁龙865带来巨大性能提升的地方。今年，该公司尚未就其DSP矢量扩展(其性能在去年的855中翻了两番)及其标量单元的改进进行沟通。然而，他们确实注意到，对于这个模块的张量加速器，与去年在Hexagon 695 DSP中推出的版本相比，他们已经实现了四倍于顶端的<strong/>,同时还能够提供比<strong>高35%的功率效率</strong>。考虑到卷积神经网络架构在从图像对象检测到自动语音识别的现代人工智能用例中的流行，这是一件大事。如上所述，这些网络中的卷积运算为每个滤波器产生矩阵输出的2D阵列，这意味着当叠加在一起时，卷积层的输出是3D阵列或张量。</p>

<p>高通还推广了他们的“新的和独特的”<strong>深度学习带宽压缩</strong>技术，这显然可以<strong>无损压缩数据约50% </strong>，进而移动一半的数据，为芯片组的其他部分释放带宽。它还应该通过减少数据吞吐量来节省电力，尽管我们没有得到任何数字，压缩数据也应该有少量的电力成本。</p>

<p>在带宽方面，骁龙865支持<strong> LPDDR5内存</strong>，这也将有利于人工智能的性能，因为它将提高资源和输入数据的传输速度。除了硬件，高通新的<strong>人工智能模型效率工具包</strong>使开发人员可以轻松压缩模型，从而节省能源。神经网络往往有大量的“冗余”参数；例如，它们可能会使隐藏层比实际需要的更宽。因此，在stage上讨论的人工智能工具包功能之一是<strong>模型压缩</strong>，引用的两种方法是空间奇异值分解(SVD)和贝叶斯压缩，这两种方法都通过去除冗余节点和根据需要调整模型结构来有效地修剪神经网络。舞台上展示的另一种模型压缩技术与量化有关，它涉及到改变权重参数和激活节点计算的数值精度。</p>

<p>神经网络权值的数值精度是指用于计算的数值是作为64、32、16(半精度)还是8位值进行存储、传输和处理的。使用较低的数值精度(例如，INT8相对于FP32)会降低整体内存使用量和数据传输速度，从而允许更高的带宽和更快的推理。今天的许多深度学习应用程序已经转向8位精度模型进行推理，这听起来可能令人惊讶:难道更高的数值精度不会在分类或回归任务中实现更“准确”的预测吗？不一定；更高的数值精度，特别是在推断过程中，可能会被浪费，因为神经网络被训练成在整个训练过程中处理噪声输入或小扰动，并且给定(FP)值的低位表示上的误差足够一致地“随机”。在某种意义上，计算的低精度被网络视为另一个噪声源，预测仍然可用。撇开启发式解释者不谈，当你在没有考虑一些重要因素的情况下错误地量化一个模型时，你很可能会受到准确性的惩罚，这就是为什么很多研究都在这个问题上</p>

<p>回到高通人工智能工具包:通过它，他们提供了<a href="https://arxiv.org/pdf/1906.04721.pdf">无数据量化</a>，允许模型在没有数据或参数微调的情况下进行量化，同时仍然在各种任务上实现接近原始模型的性能。本质上，它调整量化的权重参数，并校正切换到较低精度权重时引入的偏差误差。考虑到量化带来的好处，自动化API调用下的过程将简化模型生产和部署，高通声称在运行量化模型时<strong>的性能功耗比</strong>超过四倍。</p>

<p>但是，这并不令人震惊:量化模型可以提供巨大的带宽和存储优势。将模型转换为INT8不仅可以减少4倍的带宽，还可以获得更快的整数计算速度(取决于硬件)。毫无疑问，硬件加速的量子化和数值计算方法会带来巨大的性能提升。例如，在他的博客上，谷歌的Pete Warden写道，高通和Tensorflow团队之间的合作使8位模型在HVX DSP上运行到<strong>比在CPU上快7倍</strong> <strong>。很难夸大易于使用的量化的潜力，特别是考虑到高通是如何关注INT8性能的。</strong></p>

<p>骁龙865基于ARM的Kryo CPU仍然是人工智能引擎的重要组成部分。尽管以上段落中讨论的硬件加速更可取，但有时我们无法避免应用程序没有正确利用这些块，从而导致CPU回退。过去，ARM推出了旨在加速基于矩阵和向量的计算的特定指令集。在ARMv7处理器中，我们看到了ARM NEON的引入，这是一种SIMD架构扩展，支持类似DSP的指令。ARMv8.4-A微架构引入了专门针对点积的指令。</p>

<p>所有这些公布的性能提升都与我们在上一节中描述的许多工作负载有关，但也值得记住的是，这些骁龙865升级只是高通人工智能能力的最新改进。2017年，我们记录了他们通过Hexagon 685 DSP和其他芯片组更新将人工智能能力提高了三倍。去年，他们推出了张量加速器，并集成了对非线性函数的支持(就像前面提到的ReLU！)在硬件层面。他们还将矢量加速器的数量增加了一倍，并将标量处理单元的性能提高了20%。将所有这些与CPU方面的增强相结合，如ARM提供的更快的点积运算，以及GPU中额外的alu，高通最终将<em>的原始人工智能能力提高了两倍。</em></p>

<h3 id="practical-gains-and-expanded-use-cases">实际收益和扩展的使用案例</h3>
<p>与两年前相比，所有这些升级都使骁龙865的人工智能能力提高了五倍，但也许最重要的是，这些改进还带来了更好的每毫瓦性能，这是移动设备的一个关键指标。在2019年骁龙峰会上，高通给了我们几个基准，将他们的人工智能引擎与各种分类网络上的两个竞争对手进行了比较。这些数据似乎是使用跨平台基准测试应用程序AIMark收集的，该应用程序可以与苹果的A系列和华为的海思处理器进行比较。高通声称，这些结果利用了整个人工智能引擎，我们必须等到更彻底的基准测试，才能正确地理清每个组件的影响，并确定这些测试是如何进行的。比如B公司的结果是否表明CPU回落？例如，据我所知，AIMark目前在我们的Mate 30 Pro设备上没有利用麒麟990的NPU。但它确实支持骁龙神经处理引擎，所以它肯定会利用高通AI引擎的优势；鉴于这是内部测试，还不清楚基准测试是否为其竞争对手恰当地利用了正确的库或SDK。</p>

<p>还必须指出的是，高通正在有效地将骁龙865的人工智能处理能力与之前宣布或发布的芯片组进行比较。它的竞争对手很可能会在下一个周期带来类似影响的性能改进，如果是这样的话，那么从骁龙865设备上架那一刻起，高通将只能保持大约半年的冠军地位。也就是说，这些仍然表明了我们可以从骁龙865那里预期的那种颠簸。在传达即将发布的版本的性能改进和基准测试结果时，高通通常非常准确。</p>

<p><a href="https://www.xda-developers.com/qualcomm-snapdragon-865-ai-performance-machine-learning-analysis/image3-22/" rel="attachment wp-att-287314"/></p>



<p/>

<p>这些基准测试中出现的所有网络都在对来自ImageNet等数据库的图像进行分类，接收它们作为输入，并从数百个类别中输出一个。同样，它们依赖于我们在第二部分中描述的相同类型的操作，尽管它们的架构比这些示例复杂得多，并且在它们发表时被认为是最先进的解决方案。在最好的情况下，他们最接近的竞争对手每秒提供的推理次数还不到一半。</p>

<p><a href="https://www.xda-developers.com/qualcomm-snapdragon-865-ai-performance-machine-learning-analysis/image6-16/" rel="attachment wp-att-287316"/></p>



<p/>

<p>在功耗方面，高通提供了每瓦特的推断数字，以展示在给定功率下可能的人工智能处理量。在最好的情况下(MobileNet SSD)，骁龙人工智能引擎可以在相同的功率预算下提供两倍的推理数量。</p>

<p>对于移动设备来说，电源尤为重要。例如，考虑一个基于神经网络的Snapchat过滤器。实际上，提取面部信息并应用遮罩或输入变换的计算机视觉流水线只需要以每秒30或60次完成的速率运行，就可以实现流畅的体验。提高原始AI性能将使您能够获得更高分辨率的输入和输出更好看的滤镜，但为了更快的上传和降低功耗和散热，满足于高清分辨率也可能是更好的选择。在许多应用中，“更快”并不一定是“更好”，这样人们就能享受到更高的能效带来的好处。</p>

<p><a href="https://www.xda-developers.com/qualcomm-snapdragon-865-ai-performance-machine-learning-analysis/image7-13/" rel="attachment wp-att-287317"/></p>



<p/>

<p><em>在骁龙峰会的第二天，Snapchat Yurii Monastyrshyn的高级工程总监上台展示了他们最新的基于深度学习的滤波器如何在骁龙865上使用Hexagon 695 DSP通过Hexagon Direct NN大大加速。</em></p>

<p>最重要的是，<a href="https://www.xda-developers.com/firebase-3-new-capabilities-ml-kit-performance-monitoring-web-apps/">随着开发人员获得更简单的神经网络实现</a>以及更多的应用程序开始采用人工智能技术，并发用例将受到更多的关注，因为智能手机将必须同时处理多个并行人工智能管道(无论是单个应用程序处理来自不同来源的输入信号，还是许多应用程序在设备上单独运行)。虽然我们在计算DSP、GPU和CPU上看到了可观的能效提升，但高通传感中枢可以处理始终在线的用例，以极低的功耗监听触发词。它能够在低于1mA的电流下监控音频、视频和传感器馈送，使设备能够在熟悉的数字助理关键字之上发现特定的声音提示(如婴儿啼哭)。在这一点上，骁龙865不仅能够检测关键字，还能够检测谁在说它，以识别授权用户并采取相应的行动。</p>

<h3 id="more-ai-on-edge-devices">边缘设备上的更多人工智能</h3>
<p>这些改进最终会转化为用户体验的切实好处。涉及翻译、对象识别和标记、使用预测或项目推荐、自然语言理解、语音解析等的服务将获得更快运行和更低功耗的优势。拥有更高的计算预算还可以创建新的使用案例和体验，并将过去发生在云中的流程迁移到您的设备上。虽然AI作为一个术语在过去一直以可疑、欺骗甚至错误的方式使用(即使是原始设备制造商)，但你今天享受的许多服务最终都依赖于某种形式的机器学习算法。</p>

<p>但除了高通，其他芯片组制造商也在这方面快速迭代和改进。例如，990 5G采用了2+1 NPU核心设计，性能是麒麟980的2.5倍，是苹果A12的两倍。当该处理器发布时，它被证明每秒提供高达两倍于INT8 MobileNet的骁龙855的帧(推理)，这很难与高通提供的结果相吻合。另一方面，据报道，苹果A13 Bionic的矩阵乘法速度比其前代产品快6倍，并改进了其八核神经引擎设计。我们将不得不等待，直到我们能够在商业设备上对骁龙865及其当前和未来的竞争对手进行适当的测试，但很明显，这一领域的竞争永远不会停止，因为这三家公司已经投入了大量资源来改善他们的人工智能性能。</p>

 </div>


</div>    
</body>
</html>